# ğŸš€ Incremental Load with PySpark

This project demonstrates how to build an **incremental ETL pipeline** with PySpark.  
Instead of reloading the full dataset daily, the pipeline processes **only new or updated records**, making it efficient and cost-effective.

## ğŸ“Œ Use Case
- Data engineers often face huge daily datasets.
- Reloading all data is costly and slow.
- Solution: Incremental load â†’ process only new records.

## âš™ï¸ Tech Stack
- Python 3.x
- Apache Spark / PySpark
- Sample CSV data

## ğŸ“‚ Project Structure
```
spark-incremental-load/
â”‚â”€â”€ README.md
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ customers_2025-09-05.csv
â”‚   â”œâ”€â”€ customers_2025-09-06.csv
â”‚â”€â”€ scripts/
â”‚   â”œâ”€â”€ incremental_load.py
```

## ğŸ—ï¸ Incremental Logic
1. Read yesterdayâ€™s curated data (Parquet).
2. Read todayâ€™s raw data (CSV).
3. Compare using **customer_id**.
4. Append only new/updated records.

## â–¶ï¸ Run Script
```bash
spark-submit scripts/incremental_load.py
```

## ğŸ“Š Example Output
- Input:  
  - `customers_2025-09-05.csv` (yesterdayâ€™s customers)  
  - `customers_2025-09-06.csv` (todayâ€™s customers)  
- Output:  
  - Only new customers are appended to the curated dataset.

âœ… This project is for learning/demo purposes. You can extend it with S3, Delta Lake, or Airflow orchestration.
